{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important: This notebook will only work with fastai-0.7.x. Do not try to run any fastai-1.x code from this path in the repository because it will load fastai-0.7.x**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At Fast.ai we have introduced a new module called fastai.text which replaces the torchtext library that was used in our 2018 dl1 course. The fastai.text module also supersedes the fastai.nlp library but retains many of the key functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T22:23:27.207608Z",
     "start_time": "2019-03-04T22:23:27.184850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T22:23:29.854326Z",
     "start_time": "2019-03-04T22:23:27.209532Z"
    }
   },
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T22:23:31.111319Z",
     "start_time": "2019-03-04T22:23:31.055979Z"
    }
   },
   "outputs": [],
   "source": [
    "#!touch imdb_scripts/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T22:37:30.482350Z",
     "start_time": "2019-03-04T22:37:30.435920Z"
    }
   },
   "outputs": [],
   "source": [
    "from imdb_scripts.create_toks import make_csv_from_dir, copy_subset_of_files, create_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T22:35:39.917972Z",
     "start_time": "2019-03-04T22:35:39.860336Z"
    }
   },
   "outputs": [],
   "source": [
    "orig_small_data_dir = Path('/home/paperspace/text-augmentation/imdb_1k3k/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_small_data_dir = Path('/home/paperspace/text-augmentation/imdb_1k3k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data_dir = Path('/home/paperspace/text-augmentation/imdb_small_aug_es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(small_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/paperspace/text-augmentation/imdb_small_aug_bn')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#small_data_dir = Path(f'/home/paperspace/text-augmentation/imdb_small_aug_es_bn')\n",
    "shutil.copytree(orig_small_data_dir, small_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir {new_small_data_dir} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4009\r\n"
     ]
    }
   ],
   "source": [
    "!tree {new_small_data_dir} | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T23:55:53.104411Z",
     "start_time": "2019-03-04T23:55:53.046305Z"
    }
   },
   "outputs": [],
   "source": [
    "big_data_dir = Path('/home/paperspace/text-augmentation/imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T23:55:54.313057Z",
     "start_time": "2019-03-04T23:55:54.003440Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf {new_small_data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T23:55:55.787560Z",
     "start_time": "2019-03-04T23:55:54.316715Z"
    }
   },
   "outputs": [],
   "source": [
    "from imdb_scripts.imdb_small_experiment import add_aug_files, prepare_tokens_and_labels, run_experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved 2000 rows to /home/paperspace/text-augmentation/imdb_small_aug_es/train.csv\n",
      "train_df.shape: (2000, 2)\n",
      "saved 3000 rows to /home/paperspace/text-augmentation/imdb_small_aug_es/val.csv\n",
      "dir_path /home/paperspace/text-augmentation/imdb_small_aug_es chunksize 24000 n_lbls 1 lang en\n",
      "0\n",
      "0\n",
      "dir_path /home/paperspace/text-augmentation/imdb_small_aug_es max_vocab 60000 min_freq 1\n",
      "[('the', 31632), (',', 26624), ('.', 24570), ('and', 14172), ('a', 14124), ('of', 14026), ('to', 11219), ('is', 10149), ('in', 8767), ('it', 8173), ('that', 7476), ('i', 7318), ('this', 6141), ('\"', 5926), (\"'s\", 4526), ('was', 4418), ('not', 4224), ('movie', 3993), ('with', 3926), ('for', 3818), ('but', 3640), ('as', 3603), (')', 3104), ('-', 3072), ('you', 3043)]\n",
      "16543\n",
      "dir_path /home/paperspace/text-augmentation/imdb_small_aug_es; wt103_path /home/paperspace/fastai-fork/courses/dl2/wt103; cuda_id 0; pretrain_id wt103; cl 10; bs 64; backwards False dropmult 1.0; lr 0.004; preload True; bpe False;startat 0; use_clr True; notrain False; joined False early stopping True\n",
      "Loading /home/paperspace/text-augmentation/imdb_small_aug_es/tmp/trn_ids.npy and /home/paperspace/text-augmentation/imdb_small_aug_es/tmp/val_ids.npy\n",
      "data.shape\" torch.Size([9438, 64])\n",
      "data.shape\" torch.Size([13085, 64])\n",
      "Loading pretrained weights...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec6d3a85de8d4a6799ae0777de150034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                    \n",
      "    0      4.90923    4.662785   0.228288  \n",
      "Using early stopping...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22eb7c9f26414962ac0c957bb2f94919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                    \n",
      "    0      4.613962   4.418836   0.241681  \n",
      "    1      4.324632   4.343738   0.246382                    \n",
      "    2      4.044892   4.348574   0.247384                    \n",
      "    3      3.831142   4.357259   0.247551                    \n",
      "    4      3.632925   4.395025   0.245991                    \n",
      "    5      3.476367   4.413729   0.244962                    \n",
      "    6      3.373511   4.431653   0.244097                    \n",
      "Stopping - no improvement after 6 epochs                     \n",
      "    7      3.254807   4.462349   0.243224  \n",
      "Loading best model from fwd_lm\n",
      "saving to /home/paperspace/text-augmentation/imdb_small_aug_es/models/fwd_lm.h5 and /home/paperspace/text-augmentation/imdb_small_aug_es/models/fwd_lm_enc.h5\n",
      "dir_path /home/paperspace/text-augmentation/imdb_small_aug_es; cuda_id 0; lm_id ; clas_id None; bs 64; cl 20; backwards False; dropmult 1.0 unfreeze True startat 0; bpe False; use_clr True;use_regular_schedule False; use_discriminative True; last False;chain_thaw False; from_scratch False; train_file_id \n",
      "Trn lbls shape: (2000,)\n",
      "Number of labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4118153835704cc08812929f27da9393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.56883    0.445303   0.798333  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c2e18bf03d4813b5348c1236d8d70d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.527165   0.384371   0.834667  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce25ca2baeb497095d9bb3cf1418eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=20, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.43728    0.364531   0.852667  \n",
      "    1      0.412231   0.346168   0.862333                  \n",
      "    2      0.402224   0.335082   0.865667                  \n",
      "    3      0.351394   0.361464   0.857333                  \n",
      "    4      0.311143   0.456963   0.852333                  \n",
      "    5      0.272703   0.394408   0.87                      \n",
      "    6      0.262616   0.368494   0.866                     \n",
      "    7      0.222001   0.433692   0.867333                  \n",
      "    8      0.194231   0.463096   0.867667                  \n",
      "    9      0.167962   0.469938   0.855                     \n",
      "    10     0.163996   0.585042   0.857                     \n",
      "    11     0.123216   0.60219    0.875333                  \n",
      "    12     0.127467   0.70759    0.843667                  \n",
      "    13     0.108163   0.577373   0.875667                   \n",
      "    14     0.095931   0.571707   0.871                      \n",
      "    15     0.083205   0.668875   0.865                      \n",
      "    16     0.086386   0.62321    0.871667                   \n",
      "    17     0.075718   0.738579   0.872                      \n",
      "    18     0.059662   0.720836   0.877                      \n",
      "    19     0.068073   0.637706   0.88                       \n",
      "Plotting lrs...\n",
      "model_dir_path /home/paperspace/text-augmentation/imdb_small_aug_es; cuda_id 0; lm_id ; clas_id None; bs 64; backwards False; bpe False\n",
      "iterating\n",
      "Accuracy = 0.88 Confusion Matrix =\n",
      "[[1265  207]\n",
      " [ 153 1375]]\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): MultiBatchRNN(\n",
       "    (encoder): Embedding(16543, 400, padding_idx=1)\n",
       "    (encoder_with_dropout): EmbeddingDropout(\n",
       "      (embed): Embedding(16543, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDrop(\n",
       "        (module): LSTM(400, 1150)\n",
       "      )\n",
       "      (1): WeightDrop(\n",
       "        (module): LSTM(1150, 1150)\n",
       "      )\n",
       "      (2): WeightDrop(\n",
       "        (module): LSTM(1150, 400)\n",
       "      )\n",
       "    )\n",
       "    (dropouti): LockedDropout(\n",
       "    )\n",
       "    (dropouths): ModuleList(\n",
       "      (0): LockedDropout(\n",
       "      )\n",
       "      (1): LockedDropout(\n",
       "      )\n",
       "      (2): LockedDropout(\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): ModuleList(\n",
       "      (0): LinearBlock(\n",
       "        (lin): Linear(in_features=1200, out_features=50, bias=True)\n",
       "        (drop): Dropout(p=0.4)\n",
       "        (bn): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "      (1): LinearBlock(\n",
       "        (lin): Linear(in_features=50, out_features=2, bias=True)\n",
       "        (drop): Dropout(p=0.1)\n",
       "        (bn): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "es_metrics = run_experiment('es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved 1000 rows to /home/paperspace/text-augmentation/imdb_small_aug_es/train.csv\n",
      "train_df.shape: (1000, 2)\n",
      "saved 3000 rows to /home/paperspace/text-augmentation/imdb_small_aug_es/val.csv\n",
      "dir_path /home/paperspace/text-augmentation/imdb_small_aug_es chunksize 24000 n_lbls 1 lang en\n",
      "0\n",
      "0\n",
      "dir_path /home/paperspace/text-augmentation/imdb_small_aug_es max_vocab 60000 min_freq 1\n",
      "[('the', 14723), (',', 12171), ('.', 12086), ('a', 6944), ('and', 6889), ('of', 6359), ('to', 5687), ('is', 4739), ('in', 4088), ('it', 3995), ('i', 3555), ('that', 3249), ('this', 3094), ('\"', 2910), (\"'s\", 2767), ('was', 2231), ('-', 2224), ('\\n\\n', 2106), ('as', 1960), ('for', 1948), ('with', 1917), ('but', 1799), ('movie', 1798), ('film', 1544), (')', 1530)]\n",
      "10120\n",
      "dir_path /home/paperspace/text-augmentation/imdb_small_aug_es; wt103_path /home/paperspace/fastai-fork/courses/dl2/wt103; cuda_id 0; pretrain_id wt103; cl 10; bs 64; backwards False dropmult 1.0; lr 0.004; preload True; bpe False;startat 0; use_clr True; notrain False; joined False early stopping True\n",
      "Loading /home/paperspace/text-augmentation/imdb_small_aug_es/tmp/trn_ids.npy and /home/paperspace/text-augmentation/imdb_small_aug_es/tmp/val_ids.npy\n",
      "data.shape\" torch.Size([4706, 64])\n",
      "data.shape\" torch.Size([13085, 64])\n",
      "Loading pretrained weights...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd7a68e49ae42c2b7db6d3ae29475fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                    \n",
      "    0      5.08373    4.660999   0.223549  \n",
      "Using early stopping...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc91a7a65924b24a4e168c0e072c5c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                    \n",
      "    0      4.738844   4.236411   0.250996  \n",
      "    1      4.526642   4.146864   0.259414                    \n",
      "    2      4.309191   4.135665   0.258589                    \n",
      "    3      4.140317   4.132911   0.258944                    \n",
      "    4      3.986499   4.13533    0.258553                    \n",
      "    5      3.844303   4.153225   0.257649                    \n",
      "    6      3.729289   4.171803   0.256445                    \n",
      "    7      3.635973   4.176936   0.256842                    \n",
      "    8      3.551023   4.183989   0.256836                    \n",
      "Stopping - no improvement after 6 epochs                     \n",
      "    9      3.4935     4.189264   0.256795  \n",
      "Loading best model from fwd_lm\n",
      "saving to /home/paperspace/text-augmentation/imdb_small_aug_es/models/fwd_lm.h5 and /home/paperspace/text-augmentation/imdb_small_aug_es/models/fwd_lm_enc.h5\n",
      "dir_path /home/paperspace/text-augmentation/imdb_small_aug_es; cuda_id 0; lm_id ; clas_id None; bs 64; cl 20; backwards False; dropmult 1.0 unfreeze True startat 0; bpe False; use_clr True;use_regular_schedule False; use_discriminative True; last False;chain_thaw False; from_scratch False; train_file_id \n",
      "Trn lbls shape: (1000,)\n",
      "Number of labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34848d3ceb3240a2906fe20835a9b664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.584874   0.584236   0.689333  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1809235c1e0a4cc085a4e6182e3dfe06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.566434   0.434526   0.801     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9954bb2048174542b36229d24d4d0fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=20, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.47121    0.381393   0.833     \n",
      "    1      0.441214   0.410157   0.821667                  \n",
      "    2      0.43261    0.394024   0.838333                  \n",
      "    3      0.396887   0.396741   0.85                      \n",
      "    4      0.351493   0.361251   0.850667                  \n",
      "    5      0.340581   0.42348    0.857333                  \n",
      "    6      0.308578   0.379582   0.849                     \n",
      "    7      0.270831   0.473238   0.823333                  \n",
      "    8      0.234358   0.5042     0.837333                  \n",
      "    9      0.197786   0.595345   0.83                      \n",
      "    10     0.177275   0.530775   0.851667                  \n",
      "    11     0.169599   0.524401   0.856                     \n",
      "    12     0.156809   0.508222   0.855333                  \n",
      "    13     0.133695   0.556177   0.861                     \n",
      "    14     0.150946   0.567293   0.856667                  \n",
      "    15     0.134735   0.563782   0.860667                  \n",
      "    16     0.139792   0.572574   0.858333                  \n",
      "    17     0.132641   0.606148   0.856                     \n",
      "    18     0.116886   0.618374   0.857333                  \n",
      "    19     0.125973   0.600448   0.859333                  \n"
     ]
    }
   ],
   "source": [
    "baseline_metrics =  run_experiment('es', n_to_copy=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.Series(es_metrics).round(3).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved 1895 rows to /home/paperspace/text-augmentation/imdb_small_aug_bn/train.csv\n",
      "train_df.shape: (1895, 2)\n",
      "saved 3000 rows to /home/paperspace/text-augmentation/imdb_small_aug_bn/val.csv\n",
      "dir_path /home/paperspace/text-augmentation/imdb_small_aug_bn chunksize 24000 n_lbls 1 lang en\n",
      "0\n",
      "0\n",
      "dir_path /home/paperspace/text-augmentation/imdb_small_aug_bn max_vocab 60000 min_freq 1\n",
      "[('the', 26024), (',', 23662), ('.', 22439), ('and', 12979), ('a', 12409), ('of', 11278), ('to', 9840), ('is', 9182), ('it', 7576), ('in', 7393), ('i', 6776), ('this', 5659), ('\"', 5540), ('that', 5500), (\"'s\", 4599), ('was', 4254), ('not', 3954), ('-', 3845), ('for', 3588), ('but', 3454), ('movie', 3399), ('with', 3311), ('as', 3146), (')', 2858), ('\\n', 2771)]\n",
      "14231\n",
      "dir_path /home/paperspace/text-augmentation/imdb_small_aug_bn; wt103_path /home/paperspace/fastai-fork/courses/dl2/wt103; cuda_id 0; pretrain_id wt103; cl 10; bs 64; backwards False dropmult 1.0; lr 0.004; preload True; bpe False;startat 0; use_clr True; notrain False; joined False early stopping True\n",
      "Loading /home/paperspace/text-augmentation/imdb_small_aug_bn/tmp/trn_ids.npy and /home/paperspace/text-augmentation/imdb_small_aug_bn/tmp/val_ids.npy\n",
      "data.shape\" torch.Size([8512, 64])\n",
      "data.shape\" torch.Size([13085, 64])\n",
      "Loading pretrained weights...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd85c31f12e646f0aa71f582c50e5f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                    \n",
      "    0      5.027651   4.64364    0.22765   \n",
      "Using early stopping...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc175d8897b4c11bb6360bfb9785d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                    \n",
      "    0      4.708699   4.336899   0.244531  \n",
      "    1      4.477945   4.269341   0.251283                    \n",
      "    2      4.252074   4.25764    0.252137                    \n",
      "    3      4.056454   4.263483   0.2512                      \n",
      "    4      3.932449   4.255708   0.25268                     \n",
      "    5      3.775461   4.288131   0.250831                    \n",
      "    6      3.648299   4.320654   0.249243                    \n",
      "    7      3.596719   4.31328    0.24976                     \n",
      "    8      3.500336   4.33592    0.249124                    \n",
      "    9      3.462923   4.34439    0.248594                    \n",
      "Loading best model from fwd_lm\n",
      "saving to /home/paperspace/text-augmentation/imdb_small_aug_bn/models/fwd_lm.h5 and /home/paperspace/text-augmentation/imdb_small_aug_bn/models/fwd_lm_enc.h5\n",
      "dir_path /home/paperspace/text-augmentation/imdb_small_aug_bn; cuda_id 0; lm_id ; clas_id None; bs 64; cl 20; backwards False; dropmult 1.0 unfreeze True startat 0; bpe False; use_clr True;use_regular_schedule False; use_discriminative True; last False;chain_thaw False; from_scratch False; train_file_id \n",
      "Trn lbls shape: (1895,)\n",
      "Number of labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521ff1dfb189418d9c1bddd5ed02b7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.56649    0.467498   0.787333  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61366034e2e47fb81478fd1875de09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.516843   0.415754   0.813667  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234b9f06870d4a30975d9e6b0cae48d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=20, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.432849   0.369708   0.835333  \n",
      "    1      0.445248   0.377787   0.847667                  \n",
      "    2      0.429214   0.348125   0.852667                  \n",
      "    3      0.39887    0.365857   0.857                     \n",
      "    4      0.383021   0.352313   0.866667                  \n",
      "    5      0.335557   0.356211   0.862333                  \n",
      "    6      0.311222   0.388249   0.870333                  \n",
      "    7      0.264979   0.406841   0.867                     \n",
      "    8      0.226165   0.43312    0.870333                  \n",
      "    9      0.24078    0.437114   0.870667                  \n",
      "    10     0.186218   0.527322   0.871667                  \n",
      "    11     0.15246    0.51594    0.873                     \n",
      "    12     0.132403   0.517733   0.868667                  \n",
      "    13     0.124488   0.547163   0.868333                  \n",
      "    14     0.127445   0.561268   0.870667                  \n",
      "    15     0.10548    0.613252   0.868                     \n",
      "    16     0.107433   0.653663   0.870667                   \n",
      "    17     0.080685   0.74708    0.869333                   \n",
      "    18     0.093074   0.61439    0.874                      \n",
      "    19     0.087458   0.777763   0.86                       \n"
     ]
    }
   ],
   "source": [
    "bn_metrics = run_experiment('bn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved 1300 rows to /home/paperspace/text-augmentation/imdb_small_aug_es/train.csv\n",
      "train_df.shape: (1300, 2)\n",
      "saved 3000 rows to /home/paperspace/text-augmentation/imdb_small_aug_es/val.csv\n",
      "dir_path /home/paperspace/text-augmentation/imdb_small_aug_es chunksize 24000 n_lbls 1 lang en\n",
      "0\n",
      "0\n",
      "dir_path /home/paperspace/text-augmentation/imdb_small_aug_es max_vocab 60000 min_freq 1\n",
      "[('the', 19811), (',', 16494), ('.', 15991), ('a', 9142), ('and', 9083), ('of', 8692), ('to', 7383), ('is', 6460), ('in', 5414), ('it', 5297), ('i', 4743), ('that', 4588), ('this', 4060), ('\"', 3823), (\"'s\", 3267), ('was', 2914), ('with', 2529), ('movie', 2508), ('-', 2499), ('for', 2487), ('as', 2469), ('but', 2355), ('not', 2222), ('\\n\\n', 2107), (')', 2027)]\n",
      "12200\n",
      "dir_path /home/paperspace/text-augmentation/imdb_small_aug_es; wt103_path /home/paperspace/fastai-fork/courses/dl2/wt103; cuda_id 0; pretrain_id wt103; cl 10; bs 64; backwards False dropmult 1.0; lr 0.004; preload True; bpe False;startat 0; use_clr True; notrain False; joined False early stopping True\n",
      "Loading /home/paperspace/text-augmentation/imdb_small_aug_es/tmp/trn_ids.npy and /home/paperspace/text-augmentation/imdb_small_aug_es/tmp/val_ids.npy\n",
      "data.shape\" torch.Size([6152, 64])\n",
      "data.shape\" torch.Size([13085, 64])\n",
      "Loading pretrained weights...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21787a91cc6045b1a63abb9eefbf00d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                    \n",
      "    0      5.028587   4.669625   0.225144  \n",
      "Using early stopping...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb50e9ac1f12421280ae0c08c69385b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 28/86 [00:04<00:09,  6.04it/s, loss=4.73]"
     ]
    }
   ],
   "source": [
    "es_300_metrics = run_experiment('es', n_to_copy=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/paperspace/text-augmentation/imdb_small_aug_es')"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_aug_files('es', small_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_tokens_and_labels(small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T23:55:56.094327Z",
     "start_time": "2019-03-04T23:55:55.991882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved 1888 rows to /home/paperspace/text-augmentation/imdb_small_aug_bn/train.csv\n"
     ]
    }
   ],
   "source": [
    "train_csv_path = small_data_dir/'train.csv'\n",
    "make_csv_from_dir(small_data_dir/'train', train_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1888, 2)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trdf = pd.read_csv(small_data_dir/'train.csv', header=None); trdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T23:56:07.386847Z",
     "start_time": "2019-03-04T23:55:56.194179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved 1000 rows to /home/paperspace/text-augmentation/imdb_small_aug_bn/val.csv\n"
     ]
    }
   ],
   "source": [
    "make_csv_from_dir(small_data_dir/'test', small_data_dir/'val.csv')\n",
    "\n",
    "# could also shutil.scp this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T23:57:22.158730Z",
     "start_time": "2019-03-04T23:56:07.389002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_path /home/paperspace/text-augmentation/imdb_small_aug_bn chunksize 24000 n_lbls 1 lang en\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "create_toks(small_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T23:57:22.222364Z",
     "start_time": "2019-03-04T23:57:22.160858Z"
    }
   },
   "outputs": [],
   "source": [
    "from imdb_scripts.tok2id import tok2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T23:57:25.374003Z",
     "start_time": "2019-03-04T23:57:22.223909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_path /home/paperspace/text-augmentation/imdb_small_aug_bn max_vocab 60000 min_freq 1\n",
      "[('the', 24039), (',', 21679), ('.', 20009), ('and', 12258), ('a', 11557), ('of', 10591), ('to', 9263), ('is', 8443), ('it', 7160), ('i', 6497), ('in', 6460), ('this', 5271), ('\"', 4925), ('that', 4811), (\"'s\", 4081), ('was', 3705), ('not', 3694), ('-', 3625), ('movie', 3414), ('for', 3325), ('with', 3193), ('but', 3044), ('as', 2910), ('\\n', 2749), (')', 2673)]\n",
      "13379\n"
     ]
    }
   ],
   "source": [
    "tok2id(small_data_dir, max_vocab=60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T23:57:25.499080Z",
     "start_time": "2019-03-04T23:57:25.438396Z"
    }
   },
   "outputs": [],
   "source": [
    "from imdb_scripts.finetune_lm import train_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T23:37:02.204511Z",
     "start_time": "2019-03-04T23:37:02.140486Z"
    }
   },
   "outputs": [],
   "source": [
    "WT103_PATH = Path('/home/paperspace/fastai-fork/courses/dl2/wt103/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T23:37:06.511903Z",
     "start_time": "2019-03-04T23:37:06.465800Z"
    }
   },
   "outputs": [],
   "source": [
    "assert WT103_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T23:58:43.156335Z",
     "start_time": "2019-03-04T23:57:33.478620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_path /home/paperspace/text-augmentation/imdb_small_aug_bn; wt103_path /home/paperspace/fastai-fork/courses/dl2/wt103; cuda_id 0; pretrain_id wt103; cl 20; bs 64; backwards False dropmult 0.7; lr 0.0005; preload True; bpe False;startat 0; use_clr True; notrain False; joined False early stopping True\n",
      "Loading /home/paperspace/text-augmentation/imdb_small_aug_bn/tmp/trn_ids.npy and /home/paperspace/text-augmentation/imdb_small_aug_bn/tmp/val_ids.npy\n",
      "data.shape\" torch.Size([7815, 64])\n",
      "data.shape\" torch.Size([4173, 64])\n",
      "Loading pretrained weights...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe2acc7f65847b1842305c531534f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      5.056068   4.870562   0.217885  \n",
      "Using early stopping...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc15f423fadf45a3995fbd7c6131cd87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=20, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      4.841811   4.496714   0.238134  \n",
      "    1      4.682101   4.390377   0.246678                   \n",
      "    2      4.564518   4.336294   0.24985                    \n",
      "    3      4.472971   4.301852   0.251696                   \n",
      "    4      4.403175   4.274759   0.253056                  \n",
      "    5      4.338722   4.250138   0.254221                   \n",
      "    6      4.280165   4.230714   0.25481                    \n",
      "    7      4.223836   4.210303   0.25651                    \n",
      "    8      4.175479   4.203444   0.25653                    \n",
      "    9      4.140898   4.196986   0.256498                   \n",
      "    10     4.106423   4.187823   0.256439                   \n",
      "    11     4.076381   4.187675   0.256564                   \n",
      "    12     4.059074   4.186018   0.257037                   \n",
      "    13     4.017595   4.183032   0.257506                   \n",
      "    14     4.010646   4.18207    0.257662                   \n",
      "    15     3.999231   4.186276   0.257304                   \n",
      "    16     3.979655   4.180027   0.257834                   \n",
      "    17     3.957868   4.184336   0.25757                    \n",
      "    18     3.941933   4.18101    0.257789                   \n",
      "    19     3.937651   4.181205   0.257938                   \n",
      "Loading best model from fwd_lm\n",
      "saving to /home/paperspace/text-augmentation/imdb_small_aug_bn/models/fwd_lm.h5 and /home/paperspace/text-augmentation/imdb_small_aug_bn/models/fwd_lm_enc.h5\n",
      "CPU times: user 5min 7s, sys: 2min 15s, total: 7min 22s\n",
      "Wall time: 7min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_lm(small_data_dir, WT103_PATH, early_stopping=True, cl=20,\n",
    "         dropmult=0.7, train_last_layer_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imdb_scripts.train_clas import train_clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_path /home/paperspace/text-augmentation/imdb_small_aug_bn; cuda_id 0; lm_id ; clas_id None; bs 64; cl 20; backwards False; dropmult 1.0 unfreeze True startat 0; bpe False; use_clr True;use_regular_schedule False; use_discriminative True; last False;chain_thaw False; from_scratch False; train_file_id \n",
      "Trn lbls shape: (1888,)\n",
      "Number of labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70190f935a4440cab476cca7af012d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.573468   0.492192   0.771     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb914700a0e0477c9bdbd53298f308a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.514253   0.410899   0.822     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3954b47b5f84d8ca4f9a3d230a319a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=20, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.432802   0.443716   0.801     \n",
      "    1      0.412541   0.372754   0.847                     \n",
      "    2      0.386356   0.381477   0.846                     \n",
      "    3      0.32594    0.491089   0.836                     \n",
      "    4      0.29627    0.438037   0.827                     \n",
      "    5      0.264425   0.417298   0.867                     \n",
      "    6      0.236493   0.384776   0.85                      \n",
      "    7      0.211917   0.552344   0.822                     \n",
      "    8      0.169252   0.511051   0.849                     \n",
      "    9      0.142915   0.564444   0.855                     \n",
      "    10     0.144798   0.609102   0.851                     \n",
      "    11     0.120427   0.629628   0.867                     \n",
      "    12     0.112277   0.566752   0.868                     \n",
      "    13     0.103976   0.605294   0.836                     \n",
      "    14     0.088211   0.701956   0.86                       \n",
      "    15     0.083013   0.747197   0.853                      \n",
      "  5%|▌         | 3/59 [00:02<01:02,  1.12s/it, loss=0.08]  "
     ]
    }
   ],
   "source": [
    "learn = train_clas(small_data_dir, 0, bs=64, cl=20, lr=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;36m00-DO-NOT-USE-WITH-FASTAI-1.0.x.txt\u001b[0m@  logs_baseline_eval.txt\r\n",
      "\u001b[01;32mcarvana.ipynb\u001b[0m*                        logs_baseline_ulm.txt\r\n",
      "carvana-unet.ipynb                    \u001b[01;35mlr_plot.png\u001b[0m\r\n",
      "carvana-unet-lrg.ipynb                \u001b[01;34mlsun_scripts\u001b[0m/\r\n",
      "\u001b[01;34mcgan\u001b[0m/                                 \u001b[01;32mpascal.ipynb\u001b[0m*\r\n",
      "cifar10-darknet.ipynb                 \u001b[01;32mpascal-multi.ipynb\u001b[0m*\r\n",
      "cifar10-dawn.ipynb                    \u001b[01;34mppt\u001b[0m/\r\n",
      "cyclegan.ipynb                        style-transfer.ipynb\r\n",
      "devise.ipynb                          style-transfer-net.ipynb\r\n",
      "enhance.ipynb                         training_phase.ipynb\r\n",
      "\u001b[01;36mfastai\u001b[0m@                               translate.ipynb\r\n",
      "find-hard-examples.ipynb              Untitled.ipynb\r\n",
      "imdb-Copy1.ipynb                      wgan.ipynb\r\n",
      "\u001b[01;32mimdb.ipynb\u001b[0m*                           \u001b[01;34mwt103\u001b[0m/\r\n",
      "imdb-prepro.ipynb                     \u001b[01;34mxl\u001b[0m/\r\n",
      "\u001b[01;34mimdb_scripts\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8010000009536743,\n",
       " 0.847,\n",
       " 0.8459999995231628,\n",
       " 0.8360000009536743,\n",
       " 0.8270000009536743,\n",
       " 0.8670000009536744,\n",
       " 0.8500000004768371,\n",
       " 0.822,\n",
       " 0.849,\n",
       " 0.8550000009536743,\n",
       " 0.8510000009536743,\n",
       " 0.867,\n",
       " 0.8679999995231629,\n",
       " 0.8360000009536743,\n",
       " 0.86,\n",
       " 0.8530000009536743,\n",
       " 0.853,\n",
       " 0.8610000009536743,\n",
       " 0.855,\n",
       " 0.856]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.sched.rec_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8679999995231629"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(learn.sched.rec_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T23:37:08.482441Z",
     "start_time": "2019-03-04T23:37:08.437377Z"
    }
   },
   "outputs": [],
   "source": [
    "from imdb_scripts.eval_clas import eval_clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_dir_path /home/paperspace/text-augmentation/imdb_small_aug_bn; cuda_id 0; lm_id ; clas_id None; bs 64; backwards False; bpe False\n",
      "iterating\n",
      "Accuracy = 0.861 Confusion Matrix =\n",
      "[[435  61]\n",
      " [ 78 426]]\n",
      "CPU times: user 4.59 s, sys: 3.37 s, total: 7.96 s\n",
      "Wall time: 7.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "eval_clas(small_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_dir_path /home/paperspace/text-augmentation/imdb_small; cuda_id 0; lm_id ; clas_id None; bs 64; backwards False; bpe False\n",
      "Accuracy = 0.855 Confusion Matrix =\n",
      "[[424  72]\n",
      " [ 73 431]]\n",
      "CPU times: user 4.8 s, sys: 3.58 s, total: 8.38 s\n",
      "Wall time: 8.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "eval_clas(small_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_dir_path /home/paperspace/text-augmentation/imdb_small_aug; cuda_id 0; lm_id ; clas_id None; bs 64; backwards False; bpe False\n",
      "> /home/paperspace/fastai-fork/courses/dl2/imdb_scripts/eval_clas.py(44)eval_clas()\n",
      "-> val_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)\n",
      "(Pdb) c\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1518244421288/work/torch/lib/THC/generic/THCTensorCopy.c:20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/fastai-fork/courses/dl2/imdb_scripts/eval_clas.py\u001b[0m in \u001b[0;36meval_clas\u001b[0;34m(model_dir_path, val_dir, cuda_id, lm_id, clas_id, bs, backwards, bpe)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mval_lbls_sampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_lbls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_samp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mval_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_samp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai-fork/courses/dl2/fastai/core.py\u001b[0m in \u001b[0;36mto_gpu\u001b[0;34m(x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;34m'''puts pytorch variable to gpu, if cuda is available and USE_GPU is set to true. '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mUSE_GPU\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \"\"\"\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0;31m# Variables stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \"\"\"\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1518244421288/work/torch/lib/THC/generic/THCTensorCopy.c:20"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "eval_clas(small_data_dir, val_dir=Path('/home/paperspace/baseline_data/tmp/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T23:30:13.664434Z",
     "start_time": "2019-03-04T23:30:12.159622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-03-04 15:30:12--  http://files.fast.ai/models/wt103/itos_wt103.pkl\n",
      "Resolving files.fast.ai (files.fast.ai)... 67.205.15.147\n",
      "Connecting to files.fast.ai (files.fast.ai)|67.205.15.147|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4161252 (4.0M) [text/plain]\n",
      "Saving to: ‘/Users/shleifer/text-augmentation/imdb_small/models/wt103/itos_wt103.pkl’\n",
      "\n",
      "models/wt103/itos_w 100%[===================>]   3.97M  3.53MB/s    in 1.1s    \n",
      "\n",
      "2019-03-04 15:30:13 (3.53 MB/s) - ‘/Users/shleifer/text-augmentation/imdb_small/models/wt103/itos_wt103.pkl’ saved [4161252/4161252]\n",
      "\n",
      "FINISHED --2019-03-04 15:30:13--\n",
      "Total wall clock time: 1.3s\n",
      "Downloaded: 1 files, 4.0M in 1.1s (3.53 MB/s)\n"
     ]
    }
   ],
   "source": [
    "#!wget -nH -r -np -P {small_data_dir} http://files.fast.ai/models/wt103/itos_wt103.pkl  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T23:27:35.463304Z",
     "start_time": "2019-03-04T23:27:35.348051Z"
    }
   },
   "outputs": [],
   "source": [
    "#!wget -nH -r -np -P {small_data_dir} http://files.fast.ai/models/wt103/fwd_wt103.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T23:26:59.068566Z",
     "start_time": "2019-03-04T23:25:29.600969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-03-04 15:25:30--  http://files.fast.ai/models/wt103/bwd_wt103.h5\n",
      "Resolving files.fast.ai (files.fast.ai)... 67.205.15.147\n",
      "Connecting to files.fast.ai (files.fast.ai)|67.205.15.147|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 462387687 (441M) [text/plain]\n",
      "Saving to: ‘/Users/shleifer/text-augmentation/imdb_small/models/wt103/bwd_wt103.h5’\n",
      "\n",
      "models/wt103/bwd_wt 100%[===================>] 440.97M  5.28MB/s    in 89s     \n",
      "\n",
      "2019-03-04 15:26:58 (4.98 MB/s) - ‘/Users/shleifer/text-augmentation/imdb_small/models/wt103/bwd_wt103.h5’ saved [462387687/462387687]\n",
      "\n",
      "FINISHED --2019-03-04 15:26:58--\n",
      "Total wall clock time: 1m 29s\n",
      "Downloaded: 1 files, 441M in 1m 29s (4.98 MB/s)\n"
     ]
    }
   ],
   "source": [
    "#!wget -nH -r -np -P {small_data_dir} http://files.fast.ai/models/wt103/bwd_wt103.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fastai.text module introduces several custom tokens.\n",
    "\n",
    "We need to download the IMDB large movie reviews from this site: http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "Direct link : [Link](http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz) and untar it into the PATH location. We use pathlib which makes directory traveral a breeze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH=Path('data/')\n",
    "DATA_PATH.mkdir(exist_ok=True)\n",
    "#! curl -O http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
    "#! tar -xzfv aclImdb_v1.tar.gz -C {DATA_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "\n",
    "PATH=Path('data/aclImdb/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLAS_PATH=Path('data/imdb_clas/')\n",
    "CLAS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "LM_PATH=Path('data/imdb_lm/')\n",
    "LM_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imdb dataset has 3 classes. positive, negative and unsupervised(sentiment is unknown). \n",
    "There are 75k training reviews(12.5k pos, 12.5k neg, 50k unsup)\n",
    "There are 25k validation reviews(12.5k pos, 12.5k neg & no unsup)\n",
    "\n",
    "Refer to the README file in the imdb corpus for further information about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['neg', 'pos', 'unsup']\n",
    "\n",
    "def get_texts(path):\n",
    "    texts,labels = [],[]\n",
    "    for idx,label in enumerate(CLASSES):\n",
    "        for fname in (path/label).glob('*.*'):\n",
    "            texts.append(fname.open('r', encoding='utf-8').read())\n",
    "            labels.append(idx)\n",
    "    return np.array(texts),np.array(labels)\n",
    "\n",
    "trn_texts,trn_labels = get_texts(PATH/'train')\n",
    "val_texts,val_labels = get_texts(PATH/'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trn_texts),len(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['labels','text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a random permutation np array to shuffle the text reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "trn_idx = np.random.permutation(len(trn_texts))\n",
    "val_idx = np.random.permutation(len(val_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_texts = trn_texts[trn_idx]\n",
    "val_texts = val_texts[val_idx]\n",
    "\n",
    "trn_labels = trn_labels[trn_idx]\n",
    "val_labels = val_labels[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.DataFrame({'text':trn_texts, 'labels':trn_labels}, columns=col_names)\n",
    "df_val = pd.DataFrame({'text':val_texts, 'labels':val_labels}, columns=col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas dataframe is used to store text data in a newly evolving standard format of label followed by text columns. This was influenced by a paper by Yann LeCun ([Link to Paper](https://arxiv.org/pdf/1509.01626.pdf) [Link to Paper’s Datasets](https://drive.google.com/drive/u/0/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M)). Fastai adopts this new format for NLP datasets. In the case of IMDB, there is only one text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn[df_trn['labels']!=2].to_csv(CLAS_PATH/'train.csv', header=False, index=False)\n",
    "df_val.to_csv(CLAS_PATH/'test.csv', header=False, index=False)\n",
    "\n",
    "(CLAS_PATH/'classes.txt').open('w', encoding='utf-8').writelines(f'{o}\\n' for o in CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating the data for the Language Model(LM). The LM's goal is to learn the structure of the english language. It learns language by trying to predict the next word given a set of previous words(ngrams). Since the LM does not classify reviews, the labels can be ignored.\n",
    "\n",
    "The LM can benefit from all the textual data and there is no need to exclude the unsup/unclassified movie reviews.\n",
    "\n",
    "We first concat all the train(pos/neg/unsup = **75k**) and test(pos/neg=**25k**) reviews into a big chunk of **100k** reviews. And then we use sklearn splitter to divide up the 100k texts into 90% training and 10% validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_texts,val_texts = sklearn.model_selection.train_test_split(\n",
    "    np.concatenate([trn_texts,val_texts]), test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trn_texts), len(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.DataFrame({'text':trn_texts, 'labels':[0]*len(trn_texts)}, columns=col_names)\n",
    "df_val = pd.DataFrame({'text':val_texts, 'labels':[0]*len(val_texts)}, columns=col_names)\n",
    "\n",
    "df_trn.to_csv(LM_PATH/'train.csv', header=False, index=False)\n",
    "df_val.to_csv(LM_PATH/'test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we start cleaning up the messy text. There are 2 main activities we need to perform:\n",
    "\n",
    "1. Clean up extra spaces, tab chars, new ln chars and other characters and replace them with standard ones\n",
    "2. Use the awesome [spacy](http://spacy.io) library to tokenize the data. Since spacy does not provide a parallel/multicore version of the tokenizer, the fastai library adds this functionality. This parallel version uses all the cores of your CPUs and runs much faster than the serial version of the spacy tokenizer.\n",
    "\n",
    "Tokenization is the process of splitting the text into separate tokens so that each token can be assigned a unique index. This means we can convert the text into integer indexes our models can use.\n",
    "\n",
    "We use an appropriate chunksize as the tokenization process is memory intensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize=24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re1 = re.compile(r'  +')\n",
    "\n",
    "def fixup(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(df, n_lbls=1):\n",
    "    labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df[n_lbls].astype(str)\n",
    "    for i in range(n_lbls+1, len(df.columns)): texts += f' {FLD} {i-n_lbls} ' + df[i].astype(str)\n",
    "    texts = list(texts.apply(fixup).values)\n",
    "\n",
    "    tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\n",
    "    return tok, list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all(df, n_lbls):\n",
    "    tok, labels = [], []\n",
    "    for i, r in enumerate(df):\n",
    "        print(i)\n",
    "        tok_, labels_ = get_texts(r, n_lbls)\n",
    "        tok += tok_;\n",
    "        labels += labels_\n",
    "    return tok, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.read_csv(LM_PATH/'train.csv', header=None, chunksize=chunksize)\n",
    "df_val = pd.read_csv(LM_PATH/'test.csv', header=None, chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn, trn_labels = get_all(df_trn, 1)\n",
    "tok_val, val_labels = get_all(df_val, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(LM_PATH/'tmp').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(LM_PATH/'tmp'/'tok_trn.npy', tok_trn)\n",
    "np.save(LM_PATH/'tmp'/'tok_val.npy', tok_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn = np.load(LM_PATH/'tmp'/'tok_trn.npy')\n",
    "tok_val = np.load(LM_PATH/'tmp'/'tok_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = Counter(p for o in tok_trn for p in o)\n",
    "freq.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *vocab* is the **unique set of all tokens** in our dataset. The vocab provides us a way for us to simply replace each word in our datasets with a unique integer called an index.\n",
    "\n",
    "In a large corpus of data one might find some rare words which are only used a few times in the whole dataset. We discard such rare words and avoid trying to learn meaningful patterns out of them.\n",
    "\n",
    "Here we have set a minimum frequency of occurence to 2 times. It has been observed by NLP practicioners that a maximum vocab of 60k usually yields good results for classification tasks. So we set maz_vocab to 60000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 60000\n",
    "min_freq = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = [o for o,c in freq.most_common(max_vocab) if c>min_freq]\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(0, '_unk_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a reverse mapping called stoi which is useful to lookup the index of a given token. stoi also has the same number of elements as itos. We use a high performance container called [collections.defaultdict](https://docs.python.org/2/library/collections.html#collections.defaultdict) to store our stoi mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "val_lm = np.array([[stoi[o] for o in p] for p in tok_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(LM_PATH/'tmp'/'trn_ids.npy', trn_lm)\n",
    "np.save(LM_PATH/'tmp'/'val_ids.npy', val_lm)\n",
    "pickle.dump(itos, open(LM_PATH/'tmp'/'itos.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_lm = np.load(LM_PATH/'tmp'/'trn_ids.npy')\n",
    "val_lm = np.load(LM_PATH/'tmp'/'val_ids.npy')\n",
    "itos = pickle.load(open(LM_PATH/'tmp'/'itos.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs=len(itos)\n",
    "vs,len(trn_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wikitext103 conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to build an english language model for the IMDB corpus. We could start from scratch and try to learn the structure of the english language. But we use a technique called transfer learning to make this process easier. In transfer learning (a fairly recent idea for NLP) a pre-trained LM that has been trained on a large generic corpus(_like wikipedia articles_) can be used to transfer it's knowledge to a target LM and the weights can be fine-tuned.\n",
    "\n",
    "Our source LM is the wikitext103 LM created by Stephen Merity @ Salesforce research. [Link to dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\n",
    "The language model for wikitext103 (AWD LSTM) has been pre-trained and the weights can be downloaded here: [Link](http://files.fast.ai/models/wt103/). Our target LM is the IMDB LM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget -nH -r -np -P {PATH} http://files.fast.ai/models/wt103/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-trained LM weights have an embedding size of 400, 1150 hidden units and just 3 layers. We need to match these values  with the target IMDB LM so that the weights can be loaded up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz,nh,nl = 400,1150,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_PATH = PATH/'models'/'wt103'\n",
    "PRE_LM_PATH = PRE_PATH/'fwd_wt103.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the mean of the layer0 encoder weights. This can be used to assign weights to unknown tokens when we transfer to target IMDB LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_wgts = to_np(wgts['0.encoder.weight'])\n",
    "row_m = enc_wgts.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos2 = pickle.load((PRE_PATH/'itos_wt103.pkl').open('rb'))\n",
    "stoi2 = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos2)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we try to transfer the knowledge from wikitext to the IMDB LM, we match up the vocab words and their indexes. \n",
    "We use the defaultdict container once again, to assign mean weights to unknown IMDB tokens that do not exist in wikitext103."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_w = np.zeros((vs, em_sz), dtype=np.float32)\n",
    "for i,w in enumerate(itos):\n",
    "    r = stoi2[w]\n",
    "    new_w[i] = enc_wgts[r] if r>=0 else row_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now overwrite the weights into the wgts odict.\n",
    "The decoder module, which we will explore in detail is also loaded with the same weights due to an idea called weight tying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts['0.encoder.weight'] = T(new_w)\n",
    "wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))\n",
    "wgts['1.decoder.weight'] = T(np.copy(new_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the weights prepared, we are ready to create and start training our new IMDB language pytorch model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is fairly straightforward to create a new language model using the fastai library. Like every other lesson, our model will have a backbone and a custom head. The backbone in our case is the IMDB LM pre-trained with wikitext and the custom head is a linear classifier. In this section we will focus on the backbone LM and the next section will talk about the classifier custom head.\n",
    "\n",
    "bptt (*also known traditionally in NLP LM as ngrams*) in fastai LMs is approximated to a std. deviation around 70, by perturbing the sequence length on a per-batch basis. This is akin to shuffling our data in computer vision, only that in NLP we cannot shuffle inputs and we have to maintain statefulness. \n",
    "\n",
    "Since we are predicting words using ngrams, we want our next batch to line up with the end-points of the previous mini-batch's items. batch-size is constant and but the fastai library expands and contracts bptt each mini-batch using a clever stochastic implementation of a batch. (original credits attributed to [Smerity](https://twitter.com/jeremyphoward/status/980227258395770882))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd=1e-7\n",
    "bptt=70\n",
    "bs=52\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the LM is to learn to predict a word/token given a preceeding set of words(tokens). We take all the movie reviews in both the 90k training set and 10k validation set and concatenate them to form long strings of tokens. In fastai, we use the `LanguageModelLoader` to create a data loader which makes it easy to create and use bptt sized mini batches. The  `LanguageModelLoader` takes a concatenated string of tokens and returns a loader.\n",
    "\n",
    "We have a special modeldata object class for LMs called `LanguageModelData` to which we can pass the training and validation loaders and get in return the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)\n",
    "val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)\n",
    "md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We setup the dropouts for the model - these values have been chosen after experimentation. If you need to update them for custom LMs, you can change the weighting factor (0.7 here) based on the amount of data you have. For more data, you can reduce dropout factor and for small datasets, you can reduce overfitting by choosing a higher dropout factor. *No other dropout value requires tuning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first tune the last embedding layer so that the missing tokens initialized with mean weights get tuned properly. So we freeze everything except the last layer.\n",
    "\n",
    "We also keep track of the *accuracy* metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner= md.get_model(opt_fn, em_sz, nh, nl, \n",
    "    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])\n",
    "\n",
    "learner.metrics = [accuracy]\n",
    "learner.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.model.load_state_dict(wgts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set learning rates and fit our IMDB LM. We first run one epoch to tune the last layer which contains the embedding weights. This should help the missing tokens in the wikitext103 learn better weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "lrs = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we print out accuracy and keep track of how often we end up predicting the target word correctly. While this is a good metric to check, it is not part of our loss function as it can get quite bumpy. We only minimize cross-entropy loss in the LM.\n",
    "\n",
    "The exponent of the cross-entropy loss is called the perplexity of the LM. (low perplexity is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('lm_last_ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('lm_last_ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find(start_lr=lrs/10, end_lr=lrs*10, linear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the trained model weights and separately save the encoder part of the LM model as well. This will serve as our backbone in the classification task model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('lm1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_encoder('lm1_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier model is basically a linear layer custom head on top of the LM backbone. Setting up the classifier data is similar to the LM data setup except that we cannot use the unsup movie reviews this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.read_csv(CLAS_PATH/'train.csv', header=None, chunksize=chunksize)\n",
    "df_val = pd.read_csv(CLAS_PATH/'test.csv', header=None, chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn, trn_labels = get_all(df_trn, 1)\n",
    "tok_val, val_labels = get_all(df_val, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(CLAS_PATH/'tmp').mkdir(exist_ok=True)\n",
    "\n",
    "np.save(CLAS_PATH/'tmp'/'tok_trn.npy', tok_trn)\n",
    "np.save(CLAS_PATH/'tmp'/'tok_val.npy', tok_val)\n",
    "\n",
    "np.save(CLAS_PATH/'tmp'/'trn_labels.npy', trn_labels)\n",
    "np.save(CLAS_PATH/'tmp'/'val_labels.npy', val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn = np.load(CLAS_PATH/'tmp'/'tok_trn.npy')\n",
    "tok_val = np.load(CLAS_PATH/'tmp'/'tok_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb'))\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_clas = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "val_clas = np.array([[stoi[o] for o in p] for p in tok_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(CLAS_PATH/'tmp'/'trn_ids.npy', trn_clas)\n",
    "np.save(CLAS_PATH/'tmp'/'val_ids.npy', val_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our final model, a classifier which is really a custom linear head over our trained IMDB backbone. The steps to create the classifier model are similar to the ones for the LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_clas = np.load(CLAS_PATH/'tmp'/'trn_ids.npy')\n",
    "val_clas = np.load(CLAS_PATH/'tmp'/'val_ids.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'trn_labels.npy'))\n",
    "val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'val_labels.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt,em_sz,nh,nl = 70,400,1150,3\n",
    "vs = len(itos)\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "bs = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lbl = trn_labels.min()\n",
    "trn_labels -= min_lbl\n",
    "val_labels -= min_lbl\n",
    "c=int(trn_labels.max())+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the classifier, unlike LM, we need to read a movie review at a time and learn to predict the it's sentiment as pos/neg. We do not deal with equal bptt size batches, so we have to pad the sequences to the same length in each batch. To create batches of similar sized movie reviews, we use a sortish sampler method invented by [@Smerity](https://twitter.com/Smerity) and [@jekbradbury](https://twitter.com/jekbradbury)\n",
    "\n",
    "The sortishSampler cuts down the overall number of padding tokens the classifier ends up seeing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = TextDataset(trn_clas, trn_labels)\n",
    "val_ds = TextDataset(val_clas, val_labels)\n",
    "trn_samp = SortishSampler(trn_clas, key=lambda x: len(trn_clas[x]), bs=bs//2)\n",
    "val_samp = SortSampler(val_clas, key=lambda x: len(val_clas[x]))\n",
    "trn_dl = DataLoader(trn_ds, bs//2, transpose=True, num_workers=1, pad_idx=1, sampler=trn_samp)\n",
    "val_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)\n",
    "md = ModelData(PATH, trn_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 1\n",
    "dps = np.array([0.4, 0.5, 0.05, 0.3, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = get_rnn_classifier(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
    "          layers=[em_sz*3, 50, c], drops=[dps[4], 0.1],\n",
    "          dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
    "learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "learn.clip=.25\n",
    "learn.metrics = [accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=3e-3\n",
    "lrm = 2.6\n",
    "lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = 1e-7\n",
    "wd = 0\n",
    "learn.load_encoder('lm1_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find(lrs/1000)\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('clas_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('clas_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('clas_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('clas_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=14, use_clr=(32,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('clas_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous state of the art result was 94.1% accuracy (5.9% error). With bidir we get 95.4% accuracy (4.6% error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_loss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
